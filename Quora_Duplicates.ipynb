{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs Similarity Detection (NLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the submission for a task on flagging duplicate questions on the Quora dataset from Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## High level approach\n",
    "The questions will be converted to respective feature vectors using ***doc2vec*** (gensim's implementation). Then a ***Siamese network*** will be trained to predict the duplicacy of questions. The code with all the explanation is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Input, Add, Concatenate, Lambda, Dropout\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the CSV file into Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for Doc2Vec training (unsupervised learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of sentences and labels to make labelled sentences\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "label_list = list(df['qid1']) + list(df['qid2'])\n",
    "\n",
    "#Encoding to unicode\n",
    "for i,question in enumerate(questions):\n",
    "    questions[i] = str(question).encode('utf-8')\n",
    "\n",
    "#Tokenizing the sentences\n",
    "questions = [list(gensim.utils.tokenize(question, deacc=True, lower=True)) for question in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating LabeledLineSentence iterator to feed into Doc2Vec input\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, label_list, questions):\n",
    "        self.questions = questions\n",
    "        self.label_list = label_list\n",
    "    def __iter__(self):\n",
    "        for label, ques in zip(self.label_list, self.questions):\n",
    "            yield gensim.models.doc2vec.TaggedDocument(words=ques, tags=['QUES_%s' % label])\n",
    "\n",
    "it = LabeledLineSentence(label_list, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Doc2Vec model for generating feature vectors of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize\n",
    "size_of_vector = 300\n",
    "model = gensim.models.Doc2Vec(vector_size=size_of_vector, window=8, workers=16, negative=20, epochs=10, alpha=0.025, min_alpha=0.005)\n",
    "\n",
    "#Keeping only l2 normalized vectors, trick to save memory\n",
    "#model.init_sims(replace=True)\n",
    "\n",
    "#Build the vocabulary to train\n",
    "model.build_vocab(it)\n",
    "print(\"Vocab built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.train(it, start_alpha=model.alpha, end_alpha=model.min_alpha, total_examples = model.corpus_count, epochs = model.epochs)\n",
    "\n",
    "#Saving the model to disk\n",
    "model.save('TrainedModel.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"QUES_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data variables\n",
    "###### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model and the dataframe\n",
    "model = gensim.models.Doc2Vec.load(\"TrainedModel.doc2vec\")\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "#Getting final array of training data\n",
    "size_of_vector = 300\n",
    "train_count = len(df.index)\n",
    "train_data = np.zeros((train_count, 2, size_of_vector))\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    q1_label = \"QUES_\" + str(row['qid1'])\n",
    "    q2_label = \"QUES_\" + str(row['qid2'])\n",
    "    train_data[idx,0] = model[q1_label]\n",
    "    train_data[idx,1] = model[q2_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 2, 300)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = df['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up some memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling to balance classes and remove skew\n",
    "*Note:-* There are other techniques as well to deal with skewed classes. However, as this solution is for demonstration purpose, we will simply remove the extra training data in one class and balance them. This is known as undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data labelled as 0 : 255027\n",
      "Data labelled as 1 : 149263\n"
     ]
    }
   ],
   "source": [
    "class0 = len(train_labels[train_labels == 0])\n",
    "class1 = len(train_labels[train_labels == 1])\n",
    "print(\"Data labelled as 0 : {0}\\nData labelled as 1 : {1}\".format(class0,class1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing skew by undersampling\n",
    "extras = class0 - class1\n",
    "rand_idx = np.random.permutation(train_labels[train_labels == 0].index)\n",
    "del_idx = rand_idx[0:extras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.delete(train_data, del_idx, axis=0)\n",
    "train_labels = train_labels.drop(del_idx, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splitting data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(train_data, train_labels, test_size=0.15, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have completed the preprocessing of data. Now comes the real part.\n",
    "\n",
    "## The Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def cosine_distance(vests):\n",
    "    x, y = vests\n",
    "    x = K.l2_normalize(x, axis=-1)\n",
    "    y = K.l2_normalize(y, axis=-1)\n",
    "    return -K.mean(x * y, axis=-1, keepdims=True)\n",
    "\n",
    "def cos_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],1)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    return K.mean((1 - y_true)*K.square(y_pred) + y_true*K.square(K.maximum(0.0,(1 - y_pred))))\n",
    "\n",
    "\n",
    "def create_base_network(input_dim):\n",
    "    '''\n",
    "    Base network for feature extraction.\n",
    "    '''\n",
    "    input = Input(shape=(input_dim, ))\n",
    "    dense1 = Dense(300)(input)\n",
    "    bn1 = BatchNormalization()(dense1)\n",
    "    relu1 = Activation('relu')(bn1)\n",
    "\n",
    "    drop1 = Dropout(0.2)(relu1)\n",
    "    dense2 = Dense(300)(drop1)\n",
    "    bn2 = BatchNormalization()(dense2)\n",
    "    res2 = Add()([relu1, bn2])\n",
    "    relu2 = Activation('relu')(res2)\n",
    "\n",
    "    drop2 = Dropout(0.2)(relu2)\n",
    "    dense3 = Dense(300)(drop2)\n",
    "    bn3 = BatchNormalization()(dense3)\n",
    "    res3 = Add()([relu2, bn3])\n",
    "    relu3 = Activation('relu')(res3)\n",
    "    \n",
    "    drop3 = Dropout(0.2)(relu3)\n",
    "    dense4 = Dense(300)(drop3)\n",
    "    bn4 = BatchNormalization()(dense4)\n",
    "    res4 = Add()([relu3, bn4])\n",
    "    relu4 = Activation('relu')(res4)\n",
    "    \n",
    "    drop4 = Dropout(0.2)(relu4)\n",
    "    dense5 = Dense(300)(drop4)\n",
    "    bn5 = BatchNormalization()(dense5)\n",
    "    res5 = Add()([relu4, bn5])\n",
    "    relu5 = Activation('relu')(res5)\n",
    "    \n",
    "    feats = Concatenate()([relu5, relu4, relu3])\n",
    "    bn6 = BatchNormalization()(feats)\n",
    "\n",
    "    model = Model(outputs=bn6, inputs=input)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''\n",
    "    Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return np.mean(np.equal(predictions.ravel() > 0.7, labels))    #Duplicate if probability more than 50%\n",
    "\n",
    "def create_network(input_dim):\n",
    "    # network definition\n",
    "    base_network = create_base_network(input_dim)\n",
    "    \n",
    "    input_a = Input(shape=(input_dim,))\n",
    "    input_b = Input(shape=(input_dim,))\n",
    "    \n",
    "    # because we re-use the same instance `base_network`,\n",
    "    # the weights of the network\n",
    "    # will be shared across the two branches\n",
    "    processed_a = base_network(input_a)\n",
    "    processed_b = base_network(input_b)\n",
    "    \n",
    "    distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "    \n",
    "    prob = Dense(1, activation='sigmoid')(distance)\n",
    "    \n",
    "    model = Model(outputs=prob, inputs=[input_a, input_b])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = create_network(300)\n",
    "\n",
    "#Using Adam Optimizer with fixed 0.001 learning rate\n",
    "net.compile(loss=contrastive_loss, optimizer = Adam(lr=0.001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Epoch 1/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 45s 179us/step - loss: 0.2197 - val_loss: 0.1704\n",
      "* Accuracy on test set: 71.31%\n",
      "Real Epoch 2/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 42s 166us/step - loss: 0.1637 - val_loss: 0.1558\n",
      "* Accuracy on test set: 74.16%\n",
      "Real Epoch 3/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 42s 167us/step - loss: 0.1498 - val_loss: 0.1490\n",
      "* Accuracy on test set: 75.92%\n",
      "Real Epoch 4/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 174us/step - loss: 0.1406 - val_loss: 0.1471\n",
      "* Accuracy on test set: 77.99%\n",
      "Real Epoch 5/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 45s 177us/step - loss: 0.1338 - val_loss: 0.1440\n",
      "* Accuracy on test set: 78.36%\n",
      "Real Epoch 6/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 45s 177us/step - loss: 0.1283 - val_loss: 0.1417\n",
      "* Accuracy on test set: 79.00%\n",
      "Real Epoch 7/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 173us/step - loss: 0.1235 - val_loss: 0.1396\n",
      "* Accuracy on test set: 79.36%\n",
      "Real Epoch 8/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 172us/step - loss: 0.1200 - val_loss: 0.1377\n",
      "* Accuracy on test set: 79.45%\n",
      "Real Epoch 9/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.1165 - val_loss: 0.1368\n",
      "* Accuracy on test set: 79.55%\n",
      "Real Epoch 10/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.1130 - val_loss: 0.1403\n",
      "* Accuracy on test set: 80.32%\n",
      "Real Epoch 11/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 173us/step - loss: 0.1106 - val_loss: 0.1368\n",
      "* Accuracy on test set: 80.43%\n",
      "Real Epoch 12/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 171us/step - loss: 0.1078 - val_loss: 0.1373\n",
      "* Accuracy on test set: 80.13%\n",
      "Real Epoch 13/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 173us/step - loss: 0.1055 - val_loss: 0.1369\n",
      "* Accuracy on test set: 80.53%\n",
      "Real Epoch 14/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 171us/step - loss: 0.1035 - val_loss: 0.1414\n",
      "* Accuracy on test set: 80.80%\n",
      "Real Epoch 15/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 171us/step - loss: 0.1016 - val_loss: 0.1376\n",
      "* Accuracy on test set: 80.67%\n",
      "Real Epoch 16/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 170us/step - loss: 0.0997 - val_loss: 0.1393\n",
      "* Accuracy on test set: 80.64%\n",
      "Real Epoch 17/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0974 - val_loss: 0.1369\n",
      "* Accuracy on test set: 80.43%\n",
      "Real Epoch 18/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 168us/step - loss: 0.0963 - val_loss: 0.1406\n",
      "* Accuracy on test set: 80.80%\n",
      "Real Epoch 19/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0950 - val_loss: 0.1409\n",
      "* Accuracy on test set: 80.81%\n",
      "Real Epoch 20/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 168us/step - loss: 0.0935 - val_loss: 0.1387\n",
      "* Accuracy on test set: 80.82%\n",
      "Real Epoch 21/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0922 - val_loss: 0.1413\n",
      "* Accuracy on test set: 80.98%\n",
      "Real Epoch 22/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 168us/step - loss: 0.0907 - val_loss: 0.1449\n",
      "* Accuracy on test set: 81.00%\n",
      "Real Epoch 23/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 172us/step - loss: 0.0897 - val_loss: 0.1398\n",
      "* Accuracy on test set: 81.02%\n",
      "Real Epoch 24/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 170us/step - loss: 0.0885 - val_loss: 0.1439\n",
      "* Accuracy on test set: 81.08%\n",
      "Real Epoch 25/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0873 - val_loss: 0.1411\n",
      "* Accuracy on test set: 81.02%\n",
      "Real Epoch 26/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0859 - val_loss: 0.1439\n",
      "* Accuracy on test set: 81.06%\n",
      "Real Epoch 27/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0857 - val_loss: 0.1412\n",
      "* Accuracy on test set: 80.96%\n",
      "Real Epoch 28/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 158us/step - loss: 0.0843 - val_loss: 0.1423\n",
      "* Accuracy on test set: 81.11%\n",
      "Real Epoch 29/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 157us/step - loss: 0.0836 - val_loss: 0.1485\n",
      "* Accuracy on test set: 80.90%\n",
      "Real Epoch 30/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0828 - val_loss: 0.1464\n",
      "* Accuracy on test set: 80.97%\n",
      "Real Epoch 31/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0818 - val_loss: 0.1428\n",
      "* Accuracy on test set: 80.97%\n",
      "Real Epoch 32/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0808 - val_loss: 0.1472\n",
      "* Accuracy on test set: 81.07%\n",
      "Real Epoch 33/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0803 - val_loss: 0.1430\n",
      "* Accuracy on test set: 81.21%\n",
      "Real Epoch 34/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0797 - val_loss: 0.1453\n",
      "* Accuracy on test set: 81.05%\n",
      "Real Epoch 35/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0786 - val_loss: 0.1477\n",
      "* Accuracy on test set: 80.94%\n",
      "Real Epoch 36/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 159us/step - loss: 0.0778 - val_loss: 0.1458\n",
      "* Accuracy on test set: 81.08%\n",
      "Real Epoch 37/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 158us/step - loss: 0.0777 - val_loss: 0.1427\n",
      "* Accuracy on test set: 81.10%\n",
      "Real Epoch 38/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 40s 158us/step - loss: 0.0767 - val_loss: 0.1481\n",
      "* Accuracy on test set: 81.03%\n",
      "Real Epoch 39/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 41s 160us/step - loss: 0.0760 - val_loss: 0.1441\n",
      "* Accuracy on test set: 81.12%\n",
      "Real Epoch 40/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 39s 155us/step - loss: 0.0756 - val_loss: 0.1439\n",
      "* Accuracy on test set: 80.88%\n",
      "Real Epoch 41/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 39s 154us/step - loss: 0.0749 - val_loss: 0.1500\n",
      "* Accuracy on test set: 80.96%\n",
      "Real Epoch 42/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 39s 155us/step - loss: 0.0741 - val_loss: 0.1494\n",
      "* Accuracy on test set: 80.91%\n",
      "Real Epoch 43/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 169us/step - loss: 0.0733 - val_loss: 0.1450\n",
      "* Accuracy on test set: 80.97%\n",
      "Real Epoch 44/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 171us/step - loss: 0.0729 - val_loss: 0.1436\n",
      "* Accuracy on test set: 81.15%\n",
      "Real Epoch 45/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 171us/step - loss: 0.0727 - val_loss: 0.1523\n",
      "* Accuracy on test set: 80.94%\n",
      "Real Epoch 46/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 173us/step - loss: 0.0717 - val_loss: 0.1460\n",
      "* Accuracy on test set: 81.10%\n",
      "Real Epoch 47/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 171us/step - loss: 0.0713 - val_loss: 0.1462\n",
      "* Accuracy on test set: 81.18%\n",
      "Real Epoch 48/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 46s 181us/step - loss: 0.0713 - val_loss: 0.1446\n",
      "* Accuracy on test set: 81.17%\n",
      "Real Epoch 49/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 43s 170us/step - loss: 0.0705 - val_loss: 0.1499\n",
      "* Accuracy on test set: 81.00%\n",
      "Real Epoch 50/50\n",
      "Train on 253747 samples, validate on 44779 samples\n",
      "Epoch 1/1\n",
      "253747/253747 [==============================] - 44s 172us/step - loss: 0.0701 - val_loss: 0.1500\n",
      "* Accuracy on test set: 81.09%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    print('Real Epoch %d/50'%(epoch+1))\n",
    "    \n",
    "    # Added TensorBoard callbacks for graph visualization\n",
    "    net.fit([X_train[:,0,:], X_train[:,1,:]], Y_train,\n",
    "          validation_data=([X_test[:,0,:], X_test[:,1,:]], Y_test),\n",
    "          batch_size=128, epochs=1, shuffle=True, callbacks=[TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=128, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)])\n",
    "    \n",
    "    # Compute final accuracy on training and test sets\n",
    "    pred = net.predict([X_test[:,0,:], X_test[:,1,:]], batch_size=128)\n",
    "    te_acc = compute_accuracy(pred, Y_test)\n",
    "    \n",
    "    print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = net.to_json()\n",
    "with open(\"model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5 (saving the weights)\n",
    "net.save_weights(\"model3.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Till now, we have successfully trained on the training set and acheived _81.09% accuracy_ on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, We will predict the values for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = list(test_df[\"question1\"])\n",
    "q2 = list(test_df[\"question2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode to unicode\n",
    "for i,question in enumerate(q1):\n",
    "    q1[i] = str(question).encode('utf-8')\n",
    "for i,question in enumerate(q2):\n",
    "    q2[i] = str(question).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the sentences\n",
    "q1 = [list(gensim.utils.tokenize(question, deacc=True, lower=True)) for question in q1]\n",
    "q2 = [list(gensim.utils.tokenize(question, deacc=True, lower=True)) for question in q2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Doc2Vec model to use infer_vector()\n",
    "model = gensim.models.Doc2Vec.load(\"TrainedModel.doc2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size_of_vector = 300\n",
    "test_count = len(test_df.index)\n",
    "test_data_vector = np.zeros((test_count, 2, size_of_vector))\n",
    "\n",
    "for idx, q in enumerate(q1):\n",
    "    test_data_vector[idx,0] = model.infer_vector(q)\n",
    "for idx, q in enumerate(q2):\n",
    "    test_data_vector[idx,1] = model.infer_vector(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on the test data\n",
    "is_duplicate = net.predict([test_data_vector[:,0,:], test_data_vector[:,1,:]], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The above prediciton gives the distance between 2 vectors. Lesser the distance, higher the chance for them being duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.029931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.198537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.049482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.826719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.956664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  probability\n",
       "0        0     0.029931\n",
       "1        1     0.198537\n",
       "2        2     0.049482\n",
       "3        3     0.826719\n",
       "4        4     0.956664"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = np.zeros((test_count,2))\n",
    "final_data[:,0] = list(test_df['test_id'])\n",
    "final_data[:,1] = list(is_duplicate)\n",
    "\n",
    "final_df = pd.DataFrame(data = final_data, columns=[\"test_id\", \"probability\"])\n",
    "final_df[\"test_id\"] = final_df[\"test_id\"].astype(int)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the output to Submission.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"submission.csv\", encoding='utf-8', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we have calculated our predicitions and saved them to the CSV file in the prescribed format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
